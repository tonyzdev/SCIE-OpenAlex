import os
import csv
import json
import gzip
import io
import time
import random
import multiprocessing
from multiprocessing import Pool

import boto3
import botocore
from botocore.client import Config
from botocore import UNSIGNED

import pyarrow as pa
import pyarrow.parquet as pq

# ========== é…ç½® ==========
BUCKET = "bucket-openalex"  # ä½ è‡ªå·±çš„æ¡¶ï¼Œç”¨æ¥å†™è¿‡æ»¤ç»“æœ
PREFIX = "openalex/filtered_parquet_full"
ONE_UD = os.environ.get("TEST_UD")  # è‹¥è®¾ç½®ï¼Œåªå¤„ç†è¿™ä¸ª updated_date=YYYY-MM-DD

BATCH = 8000            # æ¯ä¸ª parquet é‡Œæœ€å¤šè®°å½•æ•°ï¼ˆå…ˆä¿å®ˆä¸€ç‚¹ï¼‰
WORKERS = 3             # å¤šè¿›ç¨‹ worker æ•°ï¼ˆå…ˆä¿å®ˆä¸€ç‚¹ï¼‰

PUBLIC_BUCKET = "openalex"   # OpenAlex å…¬å…±æ¡¶åç§°


# ========== è¯»ç™½åå• ==========
def load_allow():
    allow = set()
    with open("journal_id_map.csv", newline="", encoding="utf-8") as f:
        rd = csv.DictReader(f)
        for r in rd:
            if r["source_id"]:
                allow.add(r["source_id"])
    if not allow:
        raise SystemExit("journal_id_map.csv æ²¡æœ‰æœ‰æ•ˆçš„ source_id")
    return allow


ALLOW = load_allow()


# ========== å·¥å…·å‡½æ•° ==========
def source_ids_of_work(w):
    sids = []
    pl = (w.get("primary_location") or {}).get("source") or {}
    if pl.get("id"):
        sids.append(pl["id"])
    for loc in (w.get("locations") or []):
        so = (loc.get("source") or {})
        if so.get("id"):
            sids.append(so.get("id"))
    return sids


def extract_updated_date_from_path(p: str) -> str:
    parts = p.split("/")
    return next(
        (seg.split("=", 1)[1] for seg in parts if seg.startswith("updated_date=")),
        "unknown",
    )


# ========== workerï¼šå¤„ç†ä¸€ä¸ª .gz ==========
def process_one_gz(task):
    """
    task: (global_index, key)
    """
    idx, key = task
    ud = extract_updated_date_from_path(key)
    buf = []

    # æ¯ä¸ª worker è‡ªå·±çš„ clientï¼šè¯»ç”¨åŒ¿åï¼Œå†™ç”¨é»˜è®¤è§’è‰²
    s3_read = boto3.client("s3", config=Config(signature_version=UNSIGNED))
    s3_write = boto3.client("s3")

    def flush():
        if not buf:
            return
        table = pa.Table.from_pylist(buf)

        out_buf = io.BytesIO()
        pq.write_table(table, out_buf, compression="snappy")
        out_buf.seek(0)

        out_key = f"{PREFIX}/updated_date={ud}/part-{os.urandom(4).hex()}.parquet"
        s3_write.put_object(
            Bucket=BUCKET,
            Key=out_key,
            Body=out_buf.getvalue()
        )
        buf.clear()

        del table
        del out_buf

    for attempt in range(3):
        try:
            # è¿™é‡Œæ ‡è®°ä¸€ä¸‹å¼€å§‹å¤„ç†å“ªä¸ªåˆ†ç‰‡ï¼Œæ–¹ä¾¿ä¹‹åæ’æŸ¥
            if attempt == 0:
                print(f"[INFO] å¼€å§‹å¤„ç†åˆ†ç‰‡ #{idx}: {key}")

            resp = s3_read.get_object(Bucket=PUBLIC_BUCKET, Key=key)
            with resp["Body"] as body:
                with gzip.GzipFile(fileobj=body, mode="rb") as gz:
                    for raw in gz:
                        try:
                            line = raw.decode("utf-8", errors="ignore")
                            w = json.loads(line)
                        except Exception:
                            continue

                        if any(s in ALLOW for s in source_ids_of_work(w)):
                            buf.append(w)
                            if len(buf) >= BATCH:
                                flush()

            flush()
            print(f"[INFO] å®Œæˆåˆ†ç‰‡ #{idx}: {key}")
            return idx  # è¿”å› indexï¼Œæ–¹ä¾¿ä¸»è¿›ç¨‹ç»Ÿè®¡

        except botocore.exceptions.ClientError as e:
            code = e.response["Error"].get("Code")
            if code == "NoSuchKey":
                print(f"[WARN] S3 key ä¸å­˜åœ¨ï¼Œè·³è¿‡: {key}")
                return idx
            print(f"[WARN] boto3 ClientError å¤„ç† #{idx} {key} å‡ºé”™ (ç¬¬ {attempt+1}/3 æ¬¡): {e}")
            time.sleep(3 * (attempt + 1))

        except Exception as e:
            print(f"[WARN] å¤„ç† #{idx} {key} å‡ºé”™ (ç¬¬ {attempt+1}/3 æ¬¡): {repr(e)}")
            time.sleep(3 * (attempt + 1))

    print(f"[WARN] æ”¾å¼ƒåˆ†ç‰‡ #{idx}: {key}ï¼Œè¿ç»­å¤±è´¥ 3 æ¬¡")
    return idx


# ========== ä¸»ç¨‹åº ==========
def main():
    print("æ­£åœ¨åˆ—å‡º openalex/data/works/ ...")

    s3_public = boto3.client("s3", config=Config(signature_version=UNSIGNED))
    paginator = s3_public.get_paginator("list_objects_v2")

    if ONE_UD:
        prefix = f"data/works/{ONE_UD}/"
    else:
        prefix = "data/works/updated_date="

    keys = []
    for page in paginator.paginate(Bucket=PUBLIC_BUCKET, Prefix=prefix):
        for obj in page.get("Contents", []):
            if obj["Key"].endswith(".gz"):
                keys.append(obj["Key"])

    total = len(keys)
    print(f"å…±å‘ç° {total} ä¸ª gzip åˆ†ç‰‡")

    if not keys:
        print("æ²¡æœ‰åŒ¹é…åˆ°ä»»ä½• .gz æ–‡ä»¶ï¼Œé€€å‡ºã€‚")
        return

    # ğŸ” å…³é”®ï¼šæŠŠé¡ºåºéšæœºæ‰“ä¹±ï¼Œä¸å†æ¯æ¬¡éƒ½æŒ‰åŒæ ·é¡ºåºå¤„ç†
    random.shuffle(keys)

    # ä¸ºäº†ä»¥åæ’æŸ¥é—®é¢˜ï¼ŒæŠŠ (index, key) ä¸€èµ·ä¼ è¿› worker
    tasks = list(enumerate(keys))  # idx ä» 0 å¼€å§‹

    print(f"ä½¿ç”¨ {WORKERS} ä¸ªè¿›ç¨‹å¹¶è¡Œå¤„ç†ï¼ˆé¡ºåºå·²éšæœºæ‰“ä¹±ï¼‰")

    done = 0
    with Pool(processes=WORKERS) as pool:
        for _ in pool.imap_unordered(process_one_gz, tasks, chunksize=1):
            done += 1
            if done % 100 == 0 or done == total:
                print(f"[PROGRESS] å·²å®Œæˆ {done}/{total} ä¸ªåˆ†ç‰‡")

    print("DONE")


if __name__ == "__main__":
    multiprocessing.set_start_method("spawn")
    main()